{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Wildfire Data Ingestion Pipeline\n",
    "\n",
    "This notebook orchestrates the download of three data sources for 2020 California wildfire analysis:\n",
    "1. **NASA FIRMS** - Fire detection data\n",
    "2. **NOAA CDO** - Weather data (temperature, precipitation, wind)\n",
    "3. **USGS 3DEP** - Digital Elevation Model (terrain data)\n",
    "\n",
    "**Author:** Data Pipeline  \n",
    "**Date:** 2024-11-24  \n",
    "**Purpose:** Interactive data ingestion with validation and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import our download modules\n",
    "from config import (\n",
    "    NASA_FIRMS_API_KEY, NOAA_API_KEY, \n",
    "    FIRMS_DATA_DIR, NOAA_DATA_DIR, USGS_DATA_DIR,\n",
    "    CA_BBOX_W, CA_BBOX_S, CA_BBOX_E, CA_BBOX_N\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Environment setup complete\")\n",
    "print(f\"  - FIRMS data: {FIRMS_DATA_DIR}\")\n",
    "print(f\"  - NOAA data: {NOAA_DATA_DIR}\")\n",
    "print(f\"  - USGS data: {USGS_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Status Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_status():\n",
    "    \"\"\"Check what data has been downloaded\"\"\"\n",
    "    \n",
    "    # Check FIRMS data\n",
    "    firms_files = list(Path(FIRMS_DATA_DIR).glob('*.csv'))\n",
    "    firms_size = sum(f.stat().st_size for f in firms_files) / (1024**2)  # MB\n",
    "    \n",
    "    # Check NOAA data\n",
    "    noaa_files = list(Path(NOAA_DATA_DIR).glob('*.csv'))\n",
    "    noaa_size = sum(f.stat().st_size for f in noaa_files) / (1024**2)  # MB\n",
    "    \n",
    "    # Check USGS data\n",
    "    usgs_files = list(Path(USGS_DATA_DIR).glob('*.tif'))\n",
    "    usgs_valid = [f for f in usgs_files if f.stat().st_size > 1024*1024]  # > 1MB\n",
    "    usgs_size = sum(f.stat().st_size for f in usgs_valid) / (1024**2)  # MB\n",
    "    \n",
    "    # Create status dataframe\n",
    "    status = pd.DataFrame({\n",
    "        'Data Source': ['NASA FIRMS', 'NOAA Weather', 'USGS DEM'],\n",
    "        'Files': [len(firms_files), len(noaa_files), len(usgs_valid)],\n",
    "        'Size (MB)': [f'{firms_size:.1f}', f'{noaa_size:.1f}', f'{usgs_size:.1f}'],\n",
    "        'Status': [\n",
    "            '‚úÖ Complete' if len(firms_files) >= 37 else f'‚ö†Ô∏è {len(firms_files)}/37 files',\n",
    "            '‚úÖ Complete' if len(noaa_files) >= 12 else f'‚è≥ {len(noaa_files)}/12 months',\n",
    "            '‚úÖ Complete' if len(usgs_valid) >= 12 else f'‚ö†Ô∏è {len(usgs_valid)}/12 tiles'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    display(HTML('<h3>üìä Data Ingestion Status</h3>'))\n",
    "    display(status)\n",
    "    \n",
    "    return {\n",
    "        'firms': firms_files,\n",
    "        'noaa': noaa_files,\n",
    "        'usgs': usgs_valid\n",
    "    }\n",
    "\n",
    "# Run status check\n",
    "data_files = check_data_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. NASA FIRMS - Fire Detection Data\n",
    "\n",
    "Downloads fire detection data from MODIS satellites for California in 2020.  \n",
    "**Expected**: 37 files (10-day chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_nasa_firms import download_chunk, window_starts, SOURCE, WINDOW_DAYS\n",
    "\n",
    "def download_firms_interactive():\n",
    "    \"\"\"Download FIRMS data with progress bar and validation\"\"\"\n",
    "    \n",
    "    start = date(2020, 1, 1)\n",
    "    end = date(2020, 12, 31)\n",
    "    \n",
    "    chunks = list(window_starts(start, end, WINDOW_DAYS))\n",
    "    \n",
    "    print(f\"üì° Downloading NASA FIRMS data: {len(chunks)} chunks\")\n",
    "    print(f\"   Source: {SOURCE}\")\n",
    "    print(f\"   Window: {WINDOW_DAYS} days per chunk\\n\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    failed = []\n",
    "    \n",
    "    for d in tqdm(chunks, desc=\"Downloading chunks\"):\n",
    "        remaining = (end - d).days + 1\n",
    "        days = min(WINDOW_DAYS, max(1, remaining))\n",
    "        \n",
    "        try:\n",
    "            download_chunk(d, days)\n",
    "            time.sleep(0.25)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            failed.append((d, str(e)))\n",
    "            tqdm.write(f\"  ‚úó Failed: {d} - {e}\")\n",
    "    \n",
    "    # Validation\n",
    "    files = list(Path(FIRMS_DATA_DIR).glob('*.csv'))\n",
    "    if files:\n",
    "        sample = pd.read_csv(files[0])\n",
    "        total_rows = sum(len(pd.read_csv(f)) for f in files)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Download complete!\")\n",
    "    print(f\"   Files: {len(files)}\")\n",
    "    print(f\"   Total fire detections: {total_rows:,}\")\n",
    "    if failed:\n",
    "        print(f\"   Failed chunks: {len(failed)}\")\n",
    "    \n",
    "    return files\n",
    "\n",
    "# Uncomment to run download\n",
    "# firms_files = download_firms_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and Visualize FIRMS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_firms_data():\n",
    "    \"\"\"Load and analyze FIRMS fire detection data\"\"\"\n",
    "    \n",
    "    files = list(Path(FIRMS_DATA_DIR).glob('*.csv'))\n",
    "    \n",
    "    if not files:\n",
    "        print(\"‚ö†Ô∏è No FIRMS data found. Run download first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ Loading {len(files)} FIRMS files...\")\n",
    "    \n",
    "    # Load all data\n",
    "    dfs = []\n",
    "    for f in tqdm(files, desc=\"Loading files\"):\n",
    "        df = pd.read_csv(f)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    firms_df = pd.concat(dfs, ignore_index=True)\n",
    "    firms_df['acq_date'] = pd.to_datetime(firms_df['acq_date'])\n",
    "    \n",
    "    print(f\"\\nüìä FIRMS Data Summary:\")\n",
    "    print(f\"   Total detections: {len(firms_df):,}\")\n",
    "    print(f\"   Date range: {firms_df['acq_date'].min()} to {firms_df['acq_date'].max()}\")\n",
    "    print(f\"   Columns: {firms_df.columns.tolist()}\")\n",
    "    \n",
    "    # Plot fire activity over time\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    daily_fires = firms_df.groupby('acq_date').size()\n",
    "    daily_fires.plot(ax=ax, color='orangered', linewidth=1.5)\n",
    "    ax.set_title('Daily Fire Detections in California - 2020', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Number of Detections')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Peak fire days\n",
    "    print(f\"\\nüî• Top 5 fire days:\")\n",
    "    display(daily_fires.nlargest(5))\n",
    "    \n",
    "    return firms_df\n",
    "\n",
    "# Run analysis\n",
    "firms_df = analyze_firms_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. NOAA Weather Data\n",
    "\n",
    "Downloads weather data (temperature, precipitation, wind) from NOAA CDO API.  \n",
    "**Expected**: 12 files (monthly, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_noaa_weather import download_full_period, DATATYPES\n",
    "\n",
    "def download_noaa_interactive():\n",
    "    \"\"\"Download NOAA weather data month by month with progress tracking\"\"\"\n",
    "    \n",
    "    if not NOAA_API_KEY:\n",
    "        print(\"‚ùå Missing NOAA_API_KEY in config.py\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üå¶Ô∏è Downloading NOAA Weather Data\")\n",
    "    print(f\"   Variables: {', '.join(DATATYPES)}\")\n",
    "    print(f\"   Location: California (FIPS:06)\")\n",
    "    print(f\"   Period: 2020 (12 months)\\n\")\n",
    "    \n",
    "    year = 2020\n",
    "    success_count = 0\n",
    "    failed_months = []\n",
    "    \n",
    "    for month in tqdm(range(1, 13), desc=\"Downloading months\"):\n",
    "        start = date(year, month, 1)\n",
    "        if month == 12:\n",
    "            end = date(year, 12, 31)\n",
    "        else:\n",
    "            end = date(year, month + 1, 1) - timedelta(days=1)\n",
    "        \n",
    "        month_name = start.strftime('%B %Y')\n",
    "        \n",
    "        try:\n",
    "            tqdm.write(f\"  Downloading {month_name}...\")\n",
    "            download_full_period(start, end)\n",
    "            success_count += 1\n",
    "            \n",
    "            # Verify file was created\n",
    "            expected_file = Path(NOAA_DATA_DIR) / f\"noaa_weather_CA_{start.strftime('%Y-%m-%d')}.csv\"\n",
    "            if expected_file.exists():\n",
    "                size_kb = expected_file.stat().st_size / 1024\n",
    "                tqdm.write(f\"    ‚úì {month_name}: {size_kb:.1f} KB\")\n",
    "            \n",
    "            if month < 12:\n",
    "                time.sleep(3.0)  # Rate limiting between months\n",
    "                \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    ‚úó {month_name} failed: {e}\")\n",
    "            failed_months.append((month, str(e)))\n",
    "            time.sleep(5.0)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Download complete!\")\n",
    "    print(f\"   Successful: {success_count}/12 months\")\n",
    "    if failed_months:\n",
    "        print(f\"   Failed: {failed_months}\")\n",
    "    \n",
    "    return list(Path(NOAA_DATA_DIR).glob('*.csv'))\n",
    "\n",
    "# Uncomment to run download\n",
    "# noaa_files = download_noaa_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and Visualize NOAA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_noaa_data():\n",
    "    \"\"\"Load and analyze NOAA weather data\"\"\"\n",
    "    \n",
    "    files = sorted(Path(NOAA_DATA_DIR).glob('*.csv'))\n",
    "    \n",
    "    if not files:\n",
    "        print(\"‚ö†Ô∏è No NOAA data found. Run download first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ Loading {len(files)} NOAA weather files...\")\n",
    "    \n",
    "    # Load all data\n",
    "    dfs = [pd.read_csv(f) for f in tqdm(files, desc=\"Loading\")]\n",
    "    noaa_df = pd.concat(dfs, ignore_index=True)\n",
    "    noaa_df['date'] = pd.to_datetime(noaa_df['date'])\n",
    "    \n",
    "    print(f\"\\nüìä NOAA Weather Data Summary:\")\n",
    "    print(f\"   Total records: {len(noaa_df):,}\")\n",
    "    print(f\"   Date range: {noaa_df['date'].min()} to {noaa_df['date'].max()}\")\n",
    "    print(f\"   Data types: {noaa_df['datatype'].unique().tolist()}\")\n",
    "    print(f\"   Unique stations: {noaa_df['station'].nunique()}\")\n",
    "    \n",
    "    # Visualize data types distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Records per data type\n",
    "    noaa_df['datatype'].value_counts().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('Records per Weather Variable')\n",
    "    axes[0].set_xlabel('Data Type')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Records over time\n",
    "    daily_counts = noaa_df.groupby('date').size()\n",
    "    daily_counts.plot(ax=axes[1], color='green', linewidth=1)\n",
    "    axes[1].set_title('Daily Weather Observations')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Number of Records')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return noaa_df\n",
    "\n",
    "# Run analysis\n",
    "noaa_df = analyze_noaa_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. USGS Digital Elevation Model (DEM)\n",
    "\n",
    "Downloads terrain elevation data for California.  \n",
    "**Expected**: 12 GeoTIFF tiles (4x3 grid covering California)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_usgs_dem import download_dem_tile, RESOLUTION\n",
    "\n",
    "def download_usgs_interactive():\n",
    "    \"\"\"Download USGS DEM tiles with progress tracking\"\"\"\n",
    "    \n",
    "    print(f\"üóª Downloading USGS DEM Data\")\n",
    "    print(f\"   Resolution: {RESOLUTION}m (auto-adjusted if needed)\")\n",
    "    print(f\"   Coverage: California (4x3 grid = 12 tiles)\\n\")\n",
    "    \n",
    "    # Generate tile grid\n",
    "    cols, rows = 4, 3\n",
    "    lon_step = (CA_BBOX_E - CA_BBOX_W) / cols\n",
    "    lat_step = (CA_BBOX_N - CA_BBOX_S) / rows\n",
    "    \n",
    "    tiles = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            min_lon = CA_BBOX_W + (col * lon_step)\n",
    "            max_lon = CA_BBOX_W + ((col + 1) * lon_step)\n",
    "            min_lat = CA_BBOX_S + (row * lat_step)\n",
    "            max_lat = CA_BBOX_S + ((row + 1) * lat_step)\n",
    "            \n",
    "            filename = f\"california_dem_r{row}_c{col}.tif\"\n",
    "            tiles.append((filename, (min_lon, min_lat, max_lon, max_lat)))\n",
    "    \n",
    "    successful = 0\n",
    "    failed = []\n",
    "    \n",
    "    for filename, bbox in tqdm(tiles, desc=\"Downloading tiles\"):\n",
    "        try:\n",
    "            result = download_dem_tile(bbox, resolution=RESOLUTION, output_name=filename)\n",
    "            \n",
    "            # Check if file is valid (> 1MB)\n",
    "            filepath = Path(USGS_DATA_DIR) / filename\n",
    "            if filepath.exists() and filepath.stat().st_size > 1024*1024:\n",
    "                size_mb = filepath.stat().st_size / (1024**2)\n",
    "                tqdm.write(f\"  ‚úì {filename}: {size_mb:.1f} MB\")\n",
    "                successful += 1\n",
    "            else:\n",
    "                tqdm.write(f\"  ‚úó {filename}: Invalid/too small\")\n",
    "                failed.append(filename)\n",
    "                \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  ‚úó {filename}: {e}\")\n",
    "            failed.append(filename)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Download complete!\")\n",
    "    print(f\"   Successful: {successful}/12 tiles\")\n",
    "    if failed:\n",
    "        print(f\"   Failed: {len(failed)} tiles\")\n",
    "        print(f\"   Failed list: {failed[:5]}...\" if len(failed) > 5 else f\"   Failed list: {failed}\")\n",
    "    \n",
    "    return list(Path(USGS_DATA_DIR).glob('*.tif'))\n",
    "\n",
    "# Uncomment to run download\n",
    "# usgs_files = download_usgs_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate USGS DEM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_usgs_data():\n",
    "    \"\"\"Analyze downloaded USGS DEM tiles\"\"\"\n",
    "    \n",
    "    files = list(Path(USGS_DATA_DIR).glob('*.tif'))\n",
    "    valid_files = [f for f in files if f.stat().st_size > 1024*1024]  # > 1MB\n",
    "    \n",
    "    if not valid_files:\n",
    "        print(\"‚ö†Ô∏è No valid USGS DEM data found. Run download first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ Found {len(valid_files)} valid DEM tiles (out of {len(files)} total)\\n\")\n",
    "    \n",
    "    # Create coverage visualization\n",
    "    tile_info = []\n",
    "    for f in valid_files:\n",
    "        name = f.stem\n",
    "        size_mb = f.stat().st_size / (1024**2)\n",
    "        \n",
    "        # Parse row/col from filename\n",
    "        if 'r' in name and 'c' in name:\n",
    "            parts = name.split('_')\n",
    "            row = int(parts[-2][1])\n",
    "            col = int(parts[-1][1])\n",
    "            tile_info.append({'File': f.name, 'Row': row, 'Col': col, 'Size (MB)': f'{size_mb:.1f}'})\n",
    "    \n",
    "    if tile_info:\n",
    "        df = pd.DataFrame(tile_info).sort_values(['Row', 'Col'])\n",
    "        display(HTML('<h4>Downloaded DEM Tiles:</h4>'))\n",
    "        display(df)\n",
    "        \n",
    "        # Visualize grid coverage\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        coverage = np.zeros((3, 4))  # 3 rows x 4 cols\n",
    "        for tile in tile_info:\n",
    "            coverage[tile['Row'], tile['Col']] = 1\n",
    "        \n",
    "        im = ax.imshow(coverage, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "        ax.set_xticks(range(4))\n",
    "        ax.set_yticks(range(3))\n",
    "        ax.set_xlabel('Column')\n",
    "        ax.set_ylabel('Row')\n",
    "        ax.set_title('California DEM Tile Coverage (Green = Downloaded)')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(3):\n",
    "            for j in range(4):\n",
    "                status = '‚úì' if coverage[i, j] else '‚úó'\n",
    "                ax.text(j, i, status, ha='center', va='center', \n",
    "                       color='white' if coverage[i, j] else 'red', fontsize=20)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Coverage: {len(valid_files)}/12 tiles ({len(valid_files)/12*100:.1f}%)\")\n",
    "    \n",
    "    return valid_files\n",
    "\n",
    "# Run analysis\n",
    "usgs_files = analyze_usgs_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Summary and Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"Generate comprehensive data ingestion report\"\"\"\n",
    "    \n",
    "    # Check all data\n",
    "    firms_files = list(Path(FIRMS_DATA_DIR).glob('*.csv'))\n",
    "    noaa_files = list(Path(NOAA_DATA_DIR).glob('*.csv'))\n",
    "    usgs_files = [f for f in Path(USGS_DATA_DIR).glob('*.tif') \n",
    "                  if f.stat().st_size > 1024*1024]\n",
    "    \n",
    "    # Calculate sizes\n",
    "    firms_size = sum(f.stat().st_size for f in firms_files) / (1024**2)\n",
    "    noaa_size = sum(f.stat().st_size for f in noaa_files) / (1024**2)\n",
    "    usgs_size = sum(f.stat().st_size for f in usgs_files) / (1024**2)\n",
    "    total_size = firms_size + noaa_size + usgs_size\n",
    "    \n",
    "    # Generate report\n",
    "    report = f\"\"\"\n",
    "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "    ‚ïë   California Wildfire Data Ingestion Report - 2020       ‚ïë\n",
    "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    \n",
    "    üì° NASA FIRMS (Fire Detection)\n",
    "       Files: {len(firms_files)}/37\n",
    "       Size:  {firms_size:.1f} MB\n",
    "       Status: {'‚úÖ Complete' if len(firms_files) >= 37 else '‚ö†Ô∏è Incomplete'}\n",
    "    \n",
    "    üå¶Ô∏è NOAA Weather Data\n",
    "       Files: {len(noaa_files)}/12\n",
    "       Size:  {noaa_size:.1f} MB\n",
    "       Status: {'‚úÖ Complete' if len(noaa_files) >= 12 else '‚ö†Ô∏è Incomplete'}\n",
    "    \n",
    "    üóª USGS Digital Elevation Model\n",
    "       Files: {len(usgs_files)}/12\n",
    "       Size:  {usgs_size:.1f} MB\n",
    "       Status: {'‚úÖ Complete' if len(usgs_files) >= 12 else '‚ö†Ô∏è Partial'}\n",
    "    \n",
    "    üì¶ Total Dataset Size: {total_size:.1f} MB\n",
    "    \n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    Next Steps:\n",
    "      1. Data cleaning and preprocessing\n",
    "      2. Spatial alignment and temporal synchronization\n",
    "      3. Feature engineering for ML models\n",
    "      4. Exploratory data analysis and visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(report)\n",
    "    \n",
    "    # Return summary dict\n",
    "    return {\n",
    "        'firms': {'files': len(firms_files), 'size_mb': firms_size, 'complete': len(firms_files) >= 37},\n",
    "        'noaa': {'files': len(noaa_files), 'size_mb': noaa_size, 'complete': len(noaa_files) >= 12},\n",
    "        'usgs': {'files': len(usgs_files), 'size_mb': usgs_size, 'complete': len(usgs_files) >= 12},\n",
    "        'total_size_mb': total_size\n",
    "    }\n",
    "\n",
    "# Generate report\n",
    "summary = generate_final_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Re-download Failed Data\n",
    "\n",
    "If any data source is incomplete, run the appropriate cell to retry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick status check\n",
    "check_data_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-download FIRMS if needed\n",
    "# firms_files = download_firms_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-download NOAA if needed\n",
    "# noaa_files = download_noaa_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-download USGS if needed\n",
    "# usgs_files = download_usgs_interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
